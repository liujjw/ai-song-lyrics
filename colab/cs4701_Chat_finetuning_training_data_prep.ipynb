{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a06ec76c",
      "metadata": {
        "id": "a06ec76c"
      },
      "source": [
        "# Data preparation and analysis for chat model fine-tuning\n",
        "\n",
        "This notebook serves as a tool to preprocess and analyze the chat dataset used for fine-tuning a chat model.\n",
        "It checks for format errors, provides basic statistics, and estimates token counts for fine-tuning costs.\n",
        "The method shown here corresponds to the [current fine-tuning method](https://platform.openai.com/docs/guides/fine-tuning) for gpt-3.5-turbo.\n",
        "See [legacy fine-tuning](https://platform.openai.com/docs/guides/legacy-fine-tuning) for models like babbage-002 and davinci-002."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: pip install tiktoken\n",
        "\n",
        "!pip install tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9KOaARTCl5v",
        "outputId": "445a76b3-7afa-4aaa-9836-e029dedf609e"
      },
      "id": "m9KOaARTCl5v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e63973b",
      "metadata": {
        "id": "4e63973b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tiktoken # for token counting\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "013bdbc4",
      "metadata": {
        "id": "013bdbc4"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We first load the chat dataset from an [example JSONL file](https://github.com/openai/openai-cookbook/blob/main/examples/data/toy_chat_fine_tuning.jsonl)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c248ccd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c248ccd1",
        "outputId": "d4f228d3-1b3a-4522-e91a-c5cefc57d828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 725\n",
            "First example:\n",
            "{'role': 'system', 'content': 'You are a songwriter.'}\n",
            "{'role': 'user', 'content': 'I want to write a Drake song.'}\n",
            "{'role': 'assistant', 'content': 'Ok, tell me a verse of the song using some phrases.'}\n",
            "{'role': 'user', 'content': 'phones off, no calls, hit the stride'}\n",
            "{'role': 'assistant', 'content': \"I been in the crib with the phones off\\nI been at the house takin' no calls\\nI done hit the stride, got my shit going\\nIn the 6 cookin' with the wri-wri-wri-wri\"}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "10th example:\n",
            "{'role': 'system', 'content': 'You are a songwriter.'}\n",
            "{'role': 'user', 'content': 'I want to write a Drake song.'}\n",
            "{'role': 'assistant', 'content': 'Ok, tell me a verse of the song using some phrases.'}\n",
            "{'role': 'user', 'content': 'Trust me when I say I thought it all through,Tried to kill me but they still want the Views,I am very much alive on the six side'}\n",
            "{'role': 'assistant', 'content': 'Make me feel the same way that I did to you\\nTrust me when I say I thought it all through\\nTrust me when I say that shit is old moves\\nGod made himself a way from me to you\\nTried to kill me but they still want the Views\\nTried to kill me but they still want the Views\\nTried to kill me but I am very much alive on the six side\\nYeah, I said I am very much alive on the six side\\nI said I am....'}\n"
          ]
        }
      ],
      "source": [
        "data_path = \"drake.jsonl\"\n",
        "\n",
        "# Load the dataset\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)\n",
        "\n",
        "print('\\n\\n\\n')\n",
        "\n",
        "print(\"10th example:\")\n",
        "for message in dataset[9][\"messages\"]:\n",
        "    print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17903d61",
      "metadata": {
        "id": "17903d61"
      },
      "source": [
        "## Format validation\n",
        "\n",
        "We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n",
        "\n",
        "1. **Data Type Check**: Checks whether each entry in the dataset is a dictionary (`dict`). Error type: `data_type`.\n",
        "2. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`.\n",
        "3. **Message Keys Check**: Validates that each message in the `messages` list contains the keys `role` and `content`. Error type: `message_missing_key`.\n",
        "4. **Unrecognized Keys in Messages**: Logs if a message has keys other than `role`, `content`, `weight`, `function_call`, and `name`. Error type: `message_unrecognized_key`.\n",
        "5. **Role Validation**: Ensures the `role` is one of \"system\", \"user\", or \"assistant\". Error type: `unrecognized_role`.\n",
        "6. **Content Validation**: Verifies that `content` has textual data and is a string. Error type: `missing_content`.\n",
        "7. **Assistant Message Presence**: Checks that each conversation has at least one message from the assistant. Error type: `example_missing_assistant_message`.\n",
        "\n",
        "The code below performs these checks, and outputs counts for each type of error found are printed. This is useful for debugging and ensuring the dataset is ready for the next steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove any messages with errors and make 3 new cleaned files, for train, test, validation."
      ],
      "metadata": {
        "id": "fpvjFRtNE-JJ"
      },
      "id": "fpvjFRtNE-JJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_builder_chat_file_cleaned(train, test, val, examples):\n",
        "    \"\"\"\n",
        "    Each line is {messages:[{}, {}, {}]}. Each line is about a song from a particular\n",
        "    artist, chatted to completion of the song.\n",
        "\n",
        "    Args:\n",
        "    artist: string: if artist is not None, then only generate messages about a particular\n",
        "    artist.\n",
        "    \"\"\"\n",
        "    with open(train, 'w') as train, open(test, 'w') as test, open(val, 'w') as val:\n",
        "      for example in examples:\n",
        "        random_number = random.randint(0, 99)\n",
        "        if random_number < 70:\n",
        "          chat_data = example\n",
        "          json_line = json.dumps(chat_data)\n",
        "          train.write(json_line + \"\\n\")\n",
        "        elif random_number < 90:\n",
        "          chat_data = example\n",
        "          json_line = json.dumps(chat_data)\n",
        "          test.write(json_line + \"\\n\")\n",
        "        else:\n",
        "          chat_data = example\n",
        "          json_line = json.dumps(chat_data)\n",
        "          val.write(json_line + \"\\n\")"
      ],
      "metadata": {
        "id": "BseHbq1YNOdL"
      },
      "id": "BseHbq1YNOdL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "goodegs = []\n",
        "\n",
        "for ex in dataset:\n",
        "    good = True\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        good = False\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        good = False\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            good = False\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "            good = False\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            good = False\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            good = False\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        good = False\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "    if good:\n",
        "        goodegs.append(ex)\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors before cleaning:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    generate_builder_chat_file_cleaned(\"train.jsonl\", \"test.jsonl\", \"val.jsonl\", goodegs)\n",
        "else:\n",
        "    print(\"No errors found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98LQdr4MEn11",
        "outputId": "2f821b5c-36f7-4f2f-bf00-925f6158cb1d"
      },
      "id": "98LQdr4MEn11",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found errors before cleaning:\n",
            "missing_content: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaned data validation"
      ],
      "metadata": {
        "id": "U_OBW123HI88"
      },
      "id": "U_OBW123HI88"
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"train.jsonl\"\n",
        "\n",
        "# Load the dataset\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "NUM_EXAMPLES_FULL = len(dataset)\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-67ithGZHGcf",
        "outputId": "68a7c6fc-ee10-4fb1-a123-d7be9f6be77b"
      },
      "id": "-67ithGZHGcf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 486\n",
            "First example:\n",
            "{'role': 'system', 'content': 'You are a songwriter.'}\n",
            "{'role': 'user', 'content': 'I want to write a Drake song.'}\n",
            "{'role': 'assistant', 'content': 'Ok, tell me a verse of the song using some phrases.'}\n",
            "{'role': 'user', 'content': 'church on Sunday, not in Kansas anymore, Oh Lord'}\n",
            "{'role': 'assistant', 'content': \"Your momma used to live at the church on Sunday\\nYou just go to LIV after church on Sunday\\nOh Lord, oh Lord we're not in Kansas anymore\\nWe're not in Kansas anymore\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9f3ccbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9f3ccbf",
        "outputId": "dfa8082e-1514-4521-d093-f5df6d35d624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No errors found\n"
          ]
        }
      ],
      "source": [
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subsample the train, test, val split"
      ],
      "metadata": {
        "id": "w0MrjdjNO1_-"
      },
      "id": "w0MrjdjNO1_-"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1o6_pCz9Tgz",
        "outputId": "bb137bbb-612d-496e-df4a-97734c20b0e9"
      },
      "id": "e1o6_pCz9Tgz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (2023.8.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask) (24.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask) (7.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask) (3.18.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.bag as db\n",
        "import json\n",
        "\n",
        "samples = {\n",
        "    'SMALLEST': 0.01,\n",
        "    'SMALLER': 0.05,\n",
        "    'SMALL': 0.1,\n",
        "    'MEDIUM_SMALL': 0.2,\n",
        "    'MEDIUM_MEDIUM': 0.3,\n",
        "    'MEDIUM_LARGE': 0.4,\n",
        "    'LARGE': 0.5,\n",
        "    'LARGER': 0.7,\n",
        "    'EVEN_LARGER': 0.85\n",
        "}\n",
        "\n",
        "lines = db.read_text('train.jsonl')\n",
        "data = lines.map(json.loads)\n",
        "\n",
        "print(f'full sized dataset is {NUM_EXAMPLES_FULL} lines')\n",
        "for name, size in samples.items():\n",
        "  sampled_data = data.random_sample(size)\n",
        "\n",
        "  result = sampled_data.compute()\n",
        "\n",
        "  with open(f'train_{size}.jsonl', 'w') as file:\n",
        "    print(f'writing a training file with {len(result)} lines')\n",
        "    for item in result:\n",
        "      json.dump(item, file)\n",
        "      file.write('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIfSI3-29AEj",
        "outputId": "f3c6a7a4-7d00-4f2c-e988-37707083cafc"
      },
      "id": "dIfSI3-29AEj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full sized dataset is 486 lines\n",
            "writing a training file with 1 lines\n",
            "writing a training file with 17 lines\n",
            "writing a training file with 61 lines\n",
            "writing a training file with 100 lines\n",
            "writing a training file with 143 lines\n",
            "writing a training file with 222 lines\n",
            "writing a training file with 233 lines\n",
            "writing a training file with 353 lines\n",
            "writing a training file with 413 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validating subsamples"
      ],
      "metadata": {
        "id": "D7KDgmoY_6bs"
      },
      "id": "D7KDgmoY_6bs"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "volXnOG1_6IJ"
      },
      "id": "volXnOG1_6IJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "981e77da",
      "metadata": {
        "id": "981e77da"
      },
      "source": [
        "## Token Counting Utilities\n",
        "\n",
        "Lets define a few helpful utilities to be used in the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4b47b5",
      "metadata": {
        "id": "8f4b47b5"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fdff67d",
      "metadata": {
        "id": "0fdff67d"
      },
      "source": [
        "## Data Warnings and Token Counts\n",
        "\n",
        "With some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n",
        "\n",
        "1. **Missing System/User Messages**: Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\n",
        "2. **Number of Messages Per Example**: Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\n",
        "3. **Total Tokens Per Example**: Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\n",
        "4. **Tokens in Assistant's Messages**: Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\n",
        "5. **Token Limit Warnings**: Checks if any examples exceed the maximum token limit (4096 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e58ee4",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52e58ee4",
        "outputId": "8a43a8a1-515f-4a5a-a1b1-ab9043bcdc9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 5, 5\n",
            "mean / median: 5.0, 5.0\n",
            "p5 / p95: 5.0, 5.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 62, 940\n",
            "mean / median: 152.31034482758622, 138.0\n",
            "p5 / p95: 85.0, 224.0\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 25, 877\n",
            "mean / median: 100.42468239564428, 83.0\n",
            "p5 / p95: 37.0, 163.0\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afb04df",
      "metadata": {
        "id": "2afb04df"
      },
      "source": [
        "## Cost Estimation\n",
        "\n",
        "In this final section, we estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb95a7ce",
      "metadata": {
        "id": "fb95a7ce",
        "outputId": "2180d3ca-cd8f-45c7-d59c-202f7d707509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has ~4306 tokens that will be charged for during training\n",
            "By default, you'll train for 20 epochs on this dataset\n",
            "By default, you'll be charged for ~86120 tokens\n"
          ]
        }
      ],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10$ per million tokens with GPT-4."
      ],
      "metadata": {
        "id": "2vmQUTkyOfMo"
      },
      "id": "2vmQUTkyOfMo"
    },
    {
      "cell_type": "markdown",
      "id": "a0ad0369",
      "metadata": {
        "id": "a0ad0369"
      },
      "source": [
        "See https://openai.com/pricing to estimate total costs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}